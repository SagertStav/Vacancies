''' Используем API HH.ru HeadHunter имеет официальный API, что позволяет получать
информацию по вакансиям в удобном формате, без необходимости парсить web-страницы сайта. Чтобы получить
вакансии достаточно выполнить http - запрос методом get по адресу https: // api.hh.ru / vacancies
и передать параметры для фильтра.Ответ вернется в формате JSON, который мы сохраним себе в виде файла для
дальнейшей работы.

Каждый файл представляет из себя страницу, как если бы Вы переключались в
веб - интерфейсе по постраничной навигации, содержащую названия
вакансий и ссылки на страницы вакансий.Давайте получим их (пояснения -
в комментариях кода): '''

# Библиотека для работы с HTTP-запросами. Будем использовать ее для обращения к API HH
import requests

# Пакет для удобной работы с данными в формате json
import json

# Модуль для работы со значением времени
import time

# Модуль для работы с операционной системой. Будем использовать для работы с файлами
import os


def getPage(page=0):
    """
    Создаем метод для получения страницы со списком вакансий.
    Аргументы:
        page - Индекс страницы, начинается с 0. Значение по умолчанию 0, т.е. первая страница
    """

    # Справочник для параметров GET-запроса
    params = {
        'text': 'NAME:Аналитик',  # Текст фильтра. В имени должно быть слово "Аналитик"
        #'name':'SQL',  # Текст фильтра. В имени должно быть слово "Аналитик"
        #'text': 'SQL',
        'area': 1,  # Поиск ощуществляется по вакансиям города Москва
        'page': page,  # Индекс страницы поиска на HH
        'per_page': 100  # Кол-во вакансий на 1 странице
    }
#https://github.com/hhru/api/blob/master/docs/vacancies.md#search
    #curl - k - H    'User-Agent: api-test-agent' 'https://api.hh.ru/vacancies'
    #curl -k -H 'User-Agent: api-test-agent' 'https://api.hh.ru/vacancies?text=java&area=1&metro=6.8'
    #req = requests.get('https://api.hh.ru/vacancies', params)  # Посылаем запрос к API
    req = requests.get('https://api.hh.ru/vacancies?text=SQL', params)  # Посылаем запрос к API
    data = req.content.decode()  # Декодируем его ответ, чтобы Кириллица отображалась корректно
    req.close()
    return data


# Считываем первые 2000 вакансий
for page in range(0, 20):

    # Преобразуем текст ответа запроса в справочник Python
    jsObj = json.loads(getPage(page))

    # Сохраняем файлы в папку {путь до текущего документа со скриптом}\docs\pagination
    # Определяем количество файлов в папке для сохранения документа с ответом запроса
    # Полученное значение используем для формирования имени документа
    nextFileName = './docs/pagination/{}.json'.format(len(os.listdir('./docs/pagination')))

    # Создаем новый документ, записываем в него ответ запроса, после закрываем
    f = open(nextFileName, mode='w', encoding='utf8')
    f.write(json.dumps(jsObj, ensure_ascii=False))
    f.close()

    # Проверка на последнюю страницу, если вакансий меньше 2000
   # if (jsObj['page'] - page) <= 1:
    #    break

    # Необязательная задержка, но чтобы не нагружать сервисы hh, оставим. 5 сек мы может подождать
    time.sleep(0.25)

print('Страницы поиска собраны')